{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AT&T Spam Detector - BLOC 5\n",
    "## Final : Building the Spam Detector\n",
    "Developed by Myriam Goyet     \n",
    "Contact : https://www.linkedin.com/in/myriamgoyet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spam detector with Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is room for performance optimization, our Baseline model is already ready for testing as a spam detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our model, let's apply the same pre-processing to a new randomly selected message:    \n",
    "We use the model and the tokenizer already trained on train set.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 00:18:20.939225: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-09 00:18:21.057697: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-09 00:18:21.559512: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-09 00:18:23.095640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import json\n",
    "\n",
    "def predict_message(model, text, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Applies preprocessing, tokenization, padding, and prediction to a single message.\n",
    "\n",
    "    Parameters:\n",
    "    model: The trained model for prediction.\n",
    "    text (str): The text message to preprocess and predict.\n",
    "    tokenizer: The tokenizer fitted on the full vocabulary.\n",
    "    max_len (int): The maximum length of the sequences.\n",
    "\n",
    "    Returns:\n",
    "    str: The prediction result, either \"Spam\" or \"Ham\".\n",
    "    \"\"\"\n",
    "    # Step 1: Clean the text\n",
    "    text = ''.join(ch for ch in text if ch.isalnum() or ch == \" \" or ch == \"'\")\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "\n",
    "    # Step 2: Lemmatize and remove stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ not in STOP_WORDS and token.text not in STOP_WORDS]\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "\n",
    "    # Step 3: Tokenization\n",
    "    sequence = tokenizer.texts_to_sequences([preprocessed_text])\n",
    "\n",
    "    # Step 4: Padding\n",
    "    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "\n",
    "    # Step 5: Prediction\n",
    "    pred = model.predict(padded_sequence)[0][0]\n",
    "    return \"Spam\" if pred > 0.5 else \"Ham\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 211ms/step\n",
      "Text : Ok lar... Joking wif u oni...\n",
      "Target : 0\n",
      "The model predicts that the message is a Ham\n",
      "Prediction correct !\n"
     ]
    }
   ],
   "source": [
    "# Exemple of use on a randomly selected message:\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df= pd.read_csv(\"AT&T_data_preprocessed.csv\")\n",
    "model = joblib.load(\"Models_trained/Baseline_model.joblib\")\n",
    "with open(\"Models_trained/tokenizer_baseline_model.json\") as f:\n",
    "    tokenizer = tokenizer_from_json(f.read())\n",
    "max_len = 100\n",
    "\n",
    "text = df[\"messages\"][1]\n",
    "target = df[\"target\"][1]\n",
    "prediction = predict_message(model, text, tokenizer, max_len)\n",
    "print('Text :',text)\n",
    "print('Target :',target)\n",
    "\n",
    "print(\"The model predicts that the message is a\",prediction)\n",
    "if prediction == \"Spam\" and target == 1:\n",
    "  print(\"Prediction correct !\")\n",
    "elif prediction == \"Ham\" and target == 0:\n",
    "  print(\"Prediction correct !\")\n",
    "else:\n",
    "  print(\"Prediction wrong !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "Text : FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, √•¬£1.50 to rcv\n",
      "Target : 1\n",
      "The model predicts that the message is a Spam\n",
      "Prediction correct !\n"
     ]
    }
   ],
   "source": [
    "# Example 2 with spam\n",
    "text = df[\"messages\"][5]\n",
    "target = df[\"target\"][5]\n",
    "prediction = predict_message(model, text, tokenizer, max_len)\n",
    "print('Text :',text)\n",
    "print('Target :',target)\n",
    "\n",
    "print(\"The model predicts that the message is a\",prediction)\n",
    "if prediction == \"Spam\" and target == 1:\n",
    "  print(\"Prediction correct !\")\n",
    "elif prediction == \"Ham\" and target == 0:\n",
    "  print(\"Prediction correct !\")\n",
    "else:\n",
    "  print(\"Prediction wrong !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "Text : Congratulations! You've won a ¬£500 gift card. Click here to claim: http://winbigprizes.co.uk. Reply STOP to opt-out.\n",
      "Target : 1\n",
      "The model predicts that the message is a Spam\n",
      "Prediction correct !\n"
     ]
    }
   ],
   "source": [
    "# Exemple 3 with new unseen message\n",
    "text = \"Congratulations! You've won a ¬£500 gift card. Click here to claim: http://winbigprizes.co.uk. Reply STOP to opt-out.\"\n",
    "target = 1\n",
    "prediction = predict_message(model, text, tokenizer, max_len)\n",
    "print('Text :',text)\n",
    "print('Target :',target)\n",
    "\n",
    "print(\"The model predicts that the message is a\",prediction)\n",
    "if prediction == \"Spam\" and target == 1:\n",
    "  print(\"Prediction correct !\")\n",
    "elif prediction == \"Ham\" and target == 0:\n",
    "  print(\"Prediction correct !\")\n",
    "else:\n",
    "  print(\"Prediction wrong !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "Text : Congratulation for your Master degree ! Proud of you !.\n",
      "Target : 0\n",
      "The model predicts that the message is a Ham\n",
      "Prediction correct !\n"
     ]
    }
   ],
   "source": [
    "# Exemple 4 with new unseen message\n",
    "text = \"Congratulation for your Master degree ! Proud of you !.\"\n",
    "target = 0\n",
    "prediction = predict_message(model, text, tokenizer, max_len)\n",
    "print('Text :',text)\n",
    "print('Target :',target)\n",
    "\n",
    "print(\"The model predicts that the message is a\",prediction)\n",
    "if prediction == \"Spam\" and target == 1:\n",
    "  print(\"Prediction correct !\")\n",
    "elif prediction == \"Ham\" and target == 0:\n",
    "  print(\"Prediction correct !\")\n",
    "else:\n",
    "  print(\"Prediction wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spam detector with Best DistilBERT + Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model works the same way, but instead of having only a model and a tokenizer to load, we also have a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load SpaCy once\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = ''.join(ch for ch in text if ch.isalnum() or ch == \" \" or ch == \"'\")\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ not in STOP_WORDS]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def extract_features(text, tokenizer, model):\n",
    "    preprocessed = preprocess_text(text)\n",
    "    \n",
    "    # Tokenize & get CLS embedding\n",
    "    inputs = tokenizer(preprocessed, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # shape: (768,)\n",
    "\n",
    "    # Handcrafted features\n",
    "    num_words = len(preprocessed.split())\n",
    "    message_len = len(preprocessed)\n",
    "\n",
    "    # Final feature vector\n",
    "    full_features = np.concatenate([cls_embedding, [num_words, message_len]])  # shape: (770,)\n",
    "    return full_features.reshape(1, -1)  # shape: (1, 770)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.6.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.6.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load all components\n",
    "distilbert_model = DistilBertModel.from_pretrained(\"Models_trained/DistilBERT+lr/final_pipeline/distilbert_model\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"Models_trained/DistilBERT+lr/final_pipeline/tokenizer\")\n",
    "classifier = joblib.load(\"Models_trained/DistilBERT+lr/final_pipeline/classifier_pipeline.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "Target: Ham\n",
      "Prediction: Ham\n",
      "‚úÖ Prediction correct!\n"
     ]
    }
   ],
   "source": [
    "# Load your test message\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"AT&T_data_preprocessed.csv\")\n",
    "text = df[\"messages\"][10]\n",
    "target = df[\"target\"][10]\n",
    "\n",
    "# Extract features and predict\n",
    "X_new = extract_features(text, tokenizer, distilbert_model)\n",
    "proba = classifier.predict_proba(X_new)[0][1]\n",
    "prediction = \"Spam\" if proba > 0.5 else \"Ham\"\n",
    "\n",
    "# Output\n",
    "print(\"Text:\", text)\n",
    "print(\"Target:\", \"Spam\" if target == 1 else \"Ham\")\n",
    "print(\"Prediction:\", prediction)\n",
    "\n",
    "if (prediction == \"Spam\" and target == 1) or (prediction == \"Ham\" and target == 0):\n",
    "    print(\"‚úÖ Prediction correct!\")\n",
    "else:\n",
    "    print(\"‚ùå Prediction wrong.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: WINNER!! As a valued network customer you have been selected to receivea √•¬£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "Target: Spam\n",
      "Prediction: Spam\n",
      "‚úÖ Prediction correct!\n"
     ]
    }
   ],
   "source": [
    "# Example 2 with spam\n",
    "text = df[\"messages\"][8]\n",
    "target = df[\"target\"][8]\n",
    "\n",
    "# Extract features and predict\n",
    "X_new = extract_features(text, tokenizer, distilbert_model)\n",
    "proba = classifier.predict_proba(X_new)[0][1]\n",
    "prediction = \"Spam\" if proba > 0.5 else \"Ham\"\n",
    "\n",
    "# Output\n",
    "print(\"Text:\", text)\n",
    "print(\"Target:\", \"Spam\" if target == 1 else \"Ham\")\n",
    "print(\"Prediction:\", prediction)\n",
    "\n",
    "if (prediction == \"Spam\" and target == 1) or (prediction == \"Ham\" and target == 0):\n",
    "    print(\"‚úÖ Prediction correct!\")\n",
    "else:\n",
    "    print(\"‚ùå Prediction wrong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Congratulations! üéâ You've been selected as the winner of our $1,000 cash prize. Reply 'WIN' now to claim your reward. T&Cs apply.\n",
      "Target: Spam\n",
      "Prediction: Spam\n",
      "‚úÖ Prediction correct!\n"
     ]
    }
   ],
   "source": [
    "# Example 3 with new unseen message:\n",
    "text = \"Congratulations! üéâ You've been selected as the winner of our $1,000 cash prize. Reply 'WIN' now to claim your reward. T&Cs apply.\"\n",
    "target = 1\n",
    "\n",
    "# Extract features and predict\n",
    "X_new = extract_features(text, tokenizer, distilbert_model)\n",
    "proba = classifier.predict_proba(X_new)[0][1]\n",
    "prediction = \"Spam\" if proba > 0.5 else \"Ham\"\n",
    "\n",
    "# Output\n",
    "print(\"Text:\", text)\n",
    "print(\"Target:\", \"Spam\" if target == 1 else \"Ham\")\n",
    "print(\"Prediction:\", prediction)\n",
    "\n",
    "if (prediction == \"Spam\" and target == 1) or (prediction == \"Ham\" and target == 0):\n",
    "    print(\"‚úÖ Prediction correct!\")\n",
    "else:\n",
    "    print(\"‚ùå Prediction wrong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hey! Just checking in to see if you're still good for lunch tomorrow at 1 PM. Let me know \n",
      "Target: Ham\n",
      "Prediction: Ham\n",
      "‚úÖ Prediction correct!\n"
     ]
    }
   ],
   "source": [
    "# Example 4 with new unseen message:\n",
    "text = \"Hey! Just checking in to see if you're still good for lunch tomorrow at 1 PM. Let me know \"\n",
    "target = 0\n",
    "\n",
    "# Extract features and predict\n",
    "X_new = extract_features(text, tokenizer, distilbert_model)\n",
    "proba = classifier.predict_proba(X_new)[0][1]\n",
    "prediction = \"Spam\" if proba > 0.5 else \"Ham\"\n",
    "\n",
    "# Output\n",
    "print(\"Text:\", text)\n",
    "print(\"Target:\", \"Spam\" if target == 1 else \"Ham\")\n",
    "print(\"Prediction:\", prediction)\n",
    "\n",
    "if (prediction == \"Spam\" and target == 1) or (prediction == \"Ham\" and target == 0):\n",
    "    print(\"‚úÖ Prediction correct!\")\n",
    "else:\n",
    "    print(\"‚ùå Prediction wrong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
